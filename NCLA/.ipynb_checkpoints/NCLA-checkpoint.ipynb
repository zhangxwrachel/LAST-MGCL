{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(gpu=0, epochs=2000, dataset='citeseer', num_heads=4, num_layers=1, num_hidden=32, tau=1, seed=1, in_drop=0.6, attn_drop=0.5, lr=0.01, weight_decay=0.0001, negative_slope=0.2)\n",
      "GATConv层输入： 3703 32 4 0.6 0.5 0.2 False <function elu at 0x000001ED01E9EE80>\n",
      "num_layers: 1\n",
      "Epoch 0001 | Time(s) 1.2664 | TrainLoss 8.3548 \n",
      "Epoch 0002 | Time(s) 1.1998 | TrainLoss 8.1778 \n",
      "Epoch 0003 | Time(s) 1.1944 | TrainLoss 8.1104 \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32md:\\Desktop\\毕业论文\\代码\\M-VIEW代码\\NCLA-main\\NCLA.ipynb 单元格 1\u001b[0m line \u001b[0;36m1\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/Desktop/%E6%AF%95%E4%B8%9A%E8%AE%BA%E6%96%87/%E4%BB%A3%E7%A0%81/M-VIEW%E4%BB%A3%E7%A0%81/NCLA-main/NCLA.ipynb#W0sZmlsZQ%3D%3D?line=114'>115</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad() \u001b[39m#梯度清零\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/Desktop/%E6%AF%95%E4%B8%9A%E8%AE%BA%E6%96%87/%E4%BB%A3%E7%A0%81/M-VIEW%E4%BB%A3%E7%A0%81/NCLA-main/NCLA.ipynb#W0sZmlsZQ%3D%3D?line=115'>116</a>\u001b[0m heads \u001b[39m=\u001b[39m model(features)\u001b[39m#然后进行前向传播以获得模型预测\u001b[39;00m\n\u001b[1;32m--> <a href='vscode-notebook-cell:/d%3A/Desktop/%E6%AF%95%E4%B8%9A%E8%AE%BA%E6%96%87/%E4%BB%A3%E7%A0%81/M-VIEW%E4%BB%A3%E7%A0%81/NCLA-main/NCLA.ipynb#W0sZmlsZQ%3D%3D?line=116'>117</a>\u001b[0m loss \u001b[39m=\u001b[39m multihead_contrastive_loss(heads, adj, tau\u001b[39m=\u001b[39;49margs\u001b[39m.\u001b[39;49mtau)\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/Desktop/%E6%AF%95%E4%B8%9A%E8%AE%BA%E6%96%87/%E4%BB%A3%E7%A0%81/M-VIEW%E4%BB%A3%E7%A0%81/NCLA-main/NCLA.ipynb#W0sZmlsZQ%3D%3D?line=117'>118</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/Desktop/%E6%AF%95%E4%B8%9A%E8%AE%BA%E6%96%87/%E4%BB%A3%E7%A0%81/M-VIEW%E4%BB%A3%E7%A0%81/NCLA-main/NCLA.ipynb#W0sZmlsZQ%3D%3D?line=118'>119</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[1;32md:\\Desktop\\毕业论文\\代码\\M-VIEW代码\\NCLA-main\\loss.py:19\u001b[0m, in \u001b[0;36mmultihead_contrastive_loss\u001b[1;34m(heads, adj, tau)\u001b[0m\n\u001b[0;32m     17\u001b[0m loss \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(\u001b[39m0\u001b[39m, dtype\u001b[39m=\u001b[39m\u001b[39mfloat\u001b[39m, requires_grad\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m     18\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, \u001b[39mlen\u001b[39m(heads)):\n\u001b[1;32m---> 19\u001b[0m     loss \u001b[39m=\u001b[39m loss \u001b[39m+\u001b[39m contrastive_loss(heads[\u001b[39m0\u001b[39;49m], heads[i], adj, tau\u001b[39m=\u001b[39;49mtau)\n\u001b[0;32m     20\u001b[0m \u001b[39mreturn\u001b[39;00m loss \u001b[39m/\u001b[39m (\u001b[39mlen\u001b[39m(heads) \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m)\n",
      "File \u001b[1;32md:\\Desktop\\毕业论文\\代码\\M-VIEW代码\\NCLA-main\\loss.py:9\u001b[0m, in \u001b[0;36mcontrastive_loss\u001b[1;34m(z1, z2, adj, mean, tau, hidden_norm)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcontrastive_loss\u001b[39m(z1: torch\u001b[39m.\u001b[39mTensor, z2: torch\u001b[39m.\u001b[39mTensor, adj,\n\u001b[0;32m      7\u001b[0m                      mean: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m, tau: \u001b[39mfloat\u001b[39m \u001b[39m=\u001b[39m \u001b[39m1.0\u001b[39m, hidden_norm: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m):\n\u001b[0;32m      8\u001b[0m     l1 \u001b[39m=\u001b[39m nei_con_loss(z1, z2, tau, adj, hidden_norm)\n\u001b[1;32m----> 9\u001b[0m     l2 \u001b[39m=\u001b[39m nei_con_loss(z2, z1, tau, adj, hidden_norm)\n\u001b[0;32m     10\u001b[0m     ret \u001b[39m=\u001b[39m (l1 \u001b[39m+\u001b[39m l2) \u001b[39m*\u001b[39m \u001b[39m0.5\u001b[39m\n\u001b[0;32m     11\u001b[0m     ret \u001b[39m=\u001b[39m ret\u001b[39m.\u001b[39mmean() \u001b[39mif\u001b[39;00m mean \u001b[39melse\u001b[39;00m ret\u001b[39m.\u001b[39msum()\n",
      "File \u001b[1;32md:\\Desktop\\毕业论文\\代码\\M-VIEW代码\\NCLA-main\\loss.py:38\u001b[0m, in \u001b[0;36mnei_con_loss\u001b[1;34m(z1, z2, tau, adj, hidden_norm)\u001b[0m\n\u001b[0;32m     35\u001b[0m nei_count \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39msqueeze(torch\u001b[39m.\u001b[39mtensor(nei_count))\n\u001b[0;32m     37\u001b[0m f \u001b[39m=\u001b[39m \u001b[39mlambda\u001b[39;00m x: torch\u001b[39m.\u001b[39mexp(x \u001b[39m/\u001b[39m tau)\n\u001b[1;32m---> 38\u001b[0m intra_view_sim \u001b[39m=\u001b[39m f(sim(z1, z1, hidden_norm))\n\u001b[0;32m     39\u001b[0m inter_view_sim \u001b[39m=\u001b[39m f(sim(z1, z2, hidden_norm))\n\u001b[0;32m     41\u001b[0m loss \u001b[39m=\u001b[39m (inter_view_sim\u001b[39m.\u001b[39mdiag() \u001b[39m+\u001b[39m (intra_view_sim\u001b[39m.\u001b[39mmul(adj))\u001b[39m.\u001b[39msum(\u001b[39m1\u001b[39m) \u001b[39m+\u001b[39m (inter_view_sim\u001b[39m.\u001b[39mmul(adj))\u001b[39m.\u001b[39msum(\u001b[39m1\u001b[39m)) \u001b[39m/\u001b[39m (\n\u001b[0;32m     42\u001b[0m         intra_view_sim\u001b[39m.\u001b[39msum(\u001b[39m1\u001b[39m) \u001b[39m+\u001b[39m inter_view_sim\u001b[39m.\u001b[39msum(\u001b[39m1\u001b[39m) \u001b[39m-\u001b[39m intra_view_sim\u001b[39m.\u001b[39mdiag())\n",
      "File \u001b[1;32md:\\Desktop\\毕业论文\\代码\\M-VIEW代码\\NCLA-main\\loss.py:27\u001b[0m, in \u001b[0;36msim\u001b[1;34m(z1, z2, hidden_norm)\u001b[0m\n\u001b[0;32m     25\u001b[0m     z1 \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mnormalize(z1)\n\u001b[0;32m     26\u001b[0m     z2 \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mnormalize(z2)\n\u001b[1;32m---> 27\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mmm(z1, z2\u001b[39m.\u001b[39;49mt())\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import argparse\n",
    "import numpy as np\n",
    "import time\n",
    "import random\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import dgl\n",
    "from gat import GAT\n",
    "\n",
    "from utils import load_network_data, get_train_data, random_planetoid_splits\n",
    "from loss import multihead_contrastive_loss\n",
    "import warnings\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "parser = argparse.ArgumentParser(description='GAT')\n",
    "parser.add_argument(\"--gpu\", type=int, default=0,\n",
    "                    help=\"which GPU to use. Set -1 to use CPU.\")\n",
    "parser.add_argument(\"--epochs\", type=int, default=2000,\n",
    "                    help=\"number of training epochs\")\n",
    "parser.add_argument(\"--dataset\", type=str, default=\"citeseer\",\n",
    "                    help=\"which dataset for training\")\n",
    "parser.add_argument(\"--num-heads\", type=int, default=4,\n",
    "                    help=\"number of hidden attention heads\")\n",
    "parser.add_argument(\"--num-layers\", type=int, default=1,\n",
    "                    help=\"number of hidden layers\")\n",
    "parser.add_argument(\"--num-hidden\", type=int, default=32,\n",
    "                    help=\"number of hidden units\")\n",
    "parser.add_argument(\"--tau\", type=float, default=1,\n",
    "                    help=\"temperature-scales\")\n",
    "parser.add_argument(\"--seed\", type=int, default=1,\n",
    "                    help=\"random seed\")\n",
    "parser.add_argument(\"--in-drop\", type=float, default=0.6,\n",
    "                    help=\"input feature dropout\")\n",
    "parser.add_argument(\"--attn-drop\", type=float, default=0.5,\n",
    "                    help=\"attention dropout\")\n",
    "parser.add_argument(\"--lr\", type=float, default=0.01,\n",
    "                    help=\"learning rate\")\n",
    "parser.add_argument('--weight-decay', type=float, default=1e-4,\n",
    "                    help=\"weight decay\")\n",
    "parser.add_argument('--negative-slope', type=float, default=0.2,\n",
    "                    help=\"the negative slope of leaky relu\")\n",
    "\n",
    "args = parser.parse_known_args()[0]\n",
    "print(args)\n",
    "\n",
    "random.seed(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "\n",
    "adj2, features, Y = load_network_data(args.dataset)\n",
    "features[features > 0] = 1\n",
    "g = dgl.from_scipy(adj2)\n",
    "\n",
    "if args.gpu >= 0 and torch.cuda.is_available():\n",
    "    cuda = True\n",
    "    g = g.int().to(args.gpu)\n",
    "else:\n",
    "    cuda = False\n",
    "\n",
    "features = torch.FloatTensor(features.todense())\n",
    "f = open('NCLA_' + args.dataset + '.txt', 'a+')\n",
    "f.write('\\n\\n\\n{}\\n'.format(args))\n",
    "f.flush()\n",
    "\n",
    "labels = np.argmax(Y, 1)\n",
    "adj = torch.tensor(adj2.todense())\n",
    "\n",
    "all_time = time.time()\n",
    "num_feats = features.shape[1]\n",
    "n_classes = Y.shape[1]\n",
    "n_edges = g.number_of_edges()\n",
    "\n",
    "# add self loop\n",
    "g = dgl.remove_self_loop(g)\n",
    "g = dgl.add_self_loop(g)\n",
    "\n",
    "# create model\n",
    "heads = ([args.num_heads] * args.num_layers)\n",
    "model = GAT(g,\n",
    "            args.num_layers,\n",
    "            num_feats,\n",
    "            args.num_hidden,\n",
    "            heads,\n",
    "            F.elu,\n",
    "            args.in_drop,\n",
    "            args.attn_drop,\n",
    "            args.negative_slope)\n",
    "\n",
    "if cuda:\n",
    "    model.cuda()\n",
    "    features = features.cuda()\n",
    "    adj = adj.cuda()\n",
    "\n",
    "# use optimizer\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
    "\n",
    "# initialize graph\n",
    "dur = []\n",
    "test_acc = 0\n",
    "\n",
    "counter = 0\n",
    "min_train_loss = 100\n",
    "early_stop_counter = 100\n",
    "best_t = -1\n",
    "\n",
    "for epoch in range(args.epochs): #按照 args.epochs 指定的次数进行多轮训练。\n",
    "    if epoch >= 0:\n",
    "        t0 = time.time()   #计时\n",
    "    model.train() #模型设置为训练模式\n",
    "    optimizer.zero_grad() #梯度清零\n",
    "    heads = model(features)#然后进行前向传播以获得模型预测\n",
    "    loss = multihead_contrastive_loss(heads, adj, tau=args.tau)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        heads = model(features)\n",
    "        loss_train = multihead_contrastive_loss(heads, adj, tau=args.tau)\n",
    "\n",
    "    #如果训练损失连续 100 轮都没有下降，则进行提前停止\n",
    "    if loss_train < min_train_loss:\n",
    "        counter = 0\n",
    "        min_train_loss = loss_train\n",
    "        best_t = epoch\n",
    "        torch.save(model.state_dict(), 'best_NCLA.pkl')\n",
    "    else:\n",
    "        counter += 1\n",
    "\n",
    "    if counter >= early_stop_counter:\n",
    "        print('early stop')\n",
    "        break\n",
    "\n",
    "    if epoch >= 0:\n",
    "        dur.append(time.time() - t0)\n",
    "\n",
    "    print(\"Epoch {:04d} | Time(s) {:.4f} | TrainLoss {:.4f} \".\n",
    "          format(epoch + 1, np.mean(dur), loss_train.item()))\n",
    "\n",
    "print('Loading {}th epoch'.format(best_t))\n",
    "model.load_state_dict(torch.load('best_NCLA.pkl'))#训练结束后，从最佳轮次加载模型参数 \n",
    "model.eval()#将模型设置为评估模式，并通过模型传递特征以获得嵌入\n",
    "with torch.no_grad():\n",
    "    heads = model(features)\n",
    "embeds = torch.cat(heads, axis=1)  #从所有头部中串联嵌入\n",
    "embeds = embeds.detach().cpu()\n",
    "\n",
    "Accuaracy_test_allK = []\n",
    "numRandom = 20\n",
    "\n",
    "for train_num in [1, 2, 3, 4, 20]:\n",
    "\n",
    "    AccuaracyAll = []\n",
    "    for random_state in range(numRandom):\n",
    "        print(\n",
    "            \"\\n=============================%d-th random split with training num %d=============================\"\n",
    "            % (random_state + 1, train_num))\n",
    "\n",
    "        if train_num == 20:\n",
    "            if args.dataset in ['cora', 'citeseer', 'pubmed']:\n",
    "                # train_num per class: 20, val_num: 500, test: 1000\n",
    "                val_num = 500\n",
    "                idx_train, idx_val, idx_test = random_planetoid_splits(n_classes, torch.tensor(labels), train_num,\n",
    "                                                                       random_state)\n",
    "            else:\n",
    "                # Coauthor CS, Amazon Computers, Amazon Photo\n",
    "                # train_num per class: 20, val_num per class: 30, test: rest\n",
    "                val_num = 30\n",
    "                idx_train, idx_val, idx_test = get_train_data(Y, train_num, val_num, random_state)\n",
    "\n",
    "        else:\n",
    "            val_num = 0  # do not use a validation set when the training labels are extremely limited\n",
    "            idx_train, idx_val, idx_test = get_train_data(Y, train_num, val_num, random_state)\n",
    "\n",
    "        train_embs = embeds[idx_train, :]\n",
    "        val_embs = embeds[idx_val, :]\n",
    "        test_embs = embeds[idx_test, :]\n",
    "\n",
    "        if train_num == 20:\n",
    "            # find the best parameter C using validation set\n",
    "            best_val_score = 0.0\n",
    "            for param in [1e-4, 1e-3, 1e-2, 0.1, 1, 10, 100]:\n",
    "                LR = LogisticRegression(solver='liblinear', multi_class='ovr', random_state=0, C=param)\n",
    "                LR.fit(train_embs, labels[idx_train])\n",
    "                val_score = LR.score(val_embs, labels[idx_val])\n",
    "                if val_score > best_val_score:\n",
    "                    best_val_score = val_score\n",
    "                    best_parameters = {'C': param}\n",
    "\n",
    "            LR_best = LogisticRegression(solver='liblinear', multi_class='ovr', random_state=0, **best_parameters)\n",
    "\n",
    "            LR_best.fit(train_embs, labels[idx_train])\n",
    "            y_pred_test = LR_best.predict(test_embs)  # pred label\n",
    "            print(\"Best accuracy on validation set:{:.4f}\".format(best_val_score))\n",
    "            print(\"Best parameters:{}\".format(best_parameters))\n",
    "\n",
    "        else:  # not use a validation set when the training labels are extremely limited\n",
    "            LR = LogisticRegression(solver='liblinear', multi_class='ovr', random_state=0)\n",
    "            LR.fit(train_embs, labels[idx_train])\n",
    "            y_pred_test = LR.predict(test_embs)  # pred label\n",
    "\n",
    "        test_acc = accuracy_score(labels[idx_test], y_pred_test)\n",
    "        print(\"test accuaray:{:.4f}\".format(test_acc))\n",
    "        AccuaracyAll.append(test_acc)\n",
    "\n",
    "    average_acc = np.mean(AccuaracyAll) * 100\n",
    "    std_acc = np.std(AccuaracyAll) * 100\n",
    "    print('avg accuracy over %d random splits: %.1f +/- %.1f, for train_num: %d, val_num:%d\\n' % (\n",
    "        numRandom, average_acc, std_acc, train_num, val_num))\n",
    "    f.write('avg accuracy over %d random splits: %.1f +/- %.1f, for train_num: %d, val_num:%d\\n' % (\n",
    "        numRandom, average_acc, std_acc, train_num, val_num))\n",
    "    f.flush()\n",
    "\n",
    "    Accuaracy_test_allK.append(average_acc)\n",
    "\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(gpu=0, epochs=2000, dataset='citeseer', num_heads=4, num_layers=1, num_hidden=32, tau=1, seed=1, in_drop=0.6, attn_drop=0.5, lr=0.01, weight_decay=0.0001, negative_slope=0.2)\n",
      "GATConv层输入： 3703 32 4 0.6 0.5 0.2 False <function elu at 0x0000020C82A76980>\n",
      "Epoch 0001 | Time(s) 1.2394 | TrainLoss 8.3548 \n",
      "Loading 0th epoch\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import argparse\n",
    "import numpy as np\n",
    "import time\n",
    "import random\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import dgl\n",
    "from gat import GAT\n",
    "\n",
    "from utils import load_network_data, get_train_data, random_planetoid_splits\n",
    "from loss import multihead_contrastive_loss\n",
    "import warnings\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "parser = argparse.ArgumentParser(description='GAT')\n",
    "parser.add_argument(\"--gpu\", type=int, default=0,\n",
    "                    help=\"which GPU to use. Set -1 to use CPU.\")\n",
    "parser.add_argument(\"--epochs\", type=int, default=2000,\n",
    "                    help=\"number of training epochs\")\n",
    "parser.add_argument(\"--dataset\", type=str, default=\"citeseer\",\n",
    "                    help=\"which dataset for training\")\n",
    "parser.add_argument(\"--num-heads\", type=int, default=4,\n",
    "                    help=\"number of hidden attention heads\")\n",
    "parser.add_argument(\"--num-layers\", type=int, default=1,\n",
    "                    help=\"number of hidden layers\")\n",
    "parser.add_argument(\"--num-hidden\", type=int, default=32,\n",
    "                    help=\"number of hidden units\")\n",
    "parser.add_argument(\"--tau\", type=float, default=1,\n",
    "                    help=\"temperature-scales\")\n",
    "parser.add_argument(\"--seed\", type=int, default=1,\n",
    "                    help=\"random seed\")\n",
    "parser.add_argument(\"--in-drop\", type=float, default=0.6,\n",
    "                    help=\"input feature dropout\")\n",
    "parser.add_argument(\"--attn-drop\", type=float, default=0.5,\n",
    "                    help=\"attention dropout\")\n",
    "parser.add_argument(\"--lr\", type=float, default=0.01,\n",
    "                    help=\"learning rate\")\n",
    "parser.add_argument('--weight-decay', type=float, default=1e-4,\n",
    "                    help=\"weight decay\")\n",
    "parser.add_argument('--negative-slope', type=float, default=0.2,\n",
    "                    help=\"the negative slope of leaky relu\")\n",
    "\n",
    "args = parser.parse_known_args()[0]\n",
    "print(args)\n",
    "\n",
    "random.seed(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "\n",
    "adj2, features, Y = load_network_data(args.dataset)\n",
    "features[features > 0] = 1\n",
    "g = dgl.from_scipy(adj2)\n",
    "\n",
    "if args.gpu >= 0 and torch.cuda.is_available():\n",
    "    cuda = True\n",
    "    g = g.int().to(args.gpu)\n",
    "else:\n",
    "    cuda = False\n",
    "\n",
    "features = torch.FloatTensor(features.todense())\n",
    "f = open('NCLA_' + args.dataset + '.txt', 'a+')\n",
    "f.write('\\n\\n\\n{}\\n'.format(args))\n",
    "f.flush()\n",
    "\n",
    "labels = np.argmax(Y, 1)\n",
    "adj = torch.tensor(adj2.todense())\n",
    "\n",
    "all_time = time.time()\n",
    "num_feats = features.shape[1]\n",
    "n_classes = Y.shape[1]\n",
    "n_edges = g.number_of_edges()\n",
    "\n",
    "# add self loop\n",
    "g = dgl.remove_self_loop(g)\n",
    "g = dgl.add_self_loop(g)\n",
    "\n",
    "# create model\n",
    "heads = ([args.num_heads] * args.num_layers)\n",
    "model = GAT(g,\n",
    "            args.num_layers,\n",
    "            num_feats,\n",
    "            args.num_hidden,\n",
    "            heads,\n",
    "            F.elu,\n",
    "            args.in_drop,\n",
    "            args.attn_drop,\n",
    "            args.negative_slope)\n",
    "\n",
    "if cuda:\n",
    "    model.cuda()\n",
    "    features = features.cuda()\n",
    "    adj = adj.cuda()\n",
    "\n",
    "# use optimizer\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
    "\n",
    "# initialize graph\n",
    "dur = []\n",
    "test_acc = 0\n",
    "\n",
    "counter = 0\n",
    "min_train_loss = 100\n",
    "early_stop_counter = 100\n",
    "best_t = -1\n",
    "\n",
    "for epoch in range(1): #按照 args.epochs 指定的次数进行多轮训练。\n",
    "    if epoch >= 0:\n",
    "        t0 = time.time()   #计时\n",
    "    model.train() #模型设置为训练模式\n",
    "    optimizer.zero_grad() #梯度清零\n",
    "    heads= model(features)#然后进行前向传播以获得模型预测\n",
    "    loss = multihead_contrastive_loss(heads, adj, tau=args.tau)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        heads = model(features)\n",
    "        loss_train = multihead_contrastive_loss(heads, adj, tau=args.tau)\n",
    "\n",
    "    #如果训练损失连续 100 轮都没有下降，则进行提前停止\n",
    "    if loss_train < min_train_loss:\n",
    "        counter = 0\n",
    "        min_train_loss = loss_train\n",
    "        best_t = epoch\n",
    "        torch.save(model.state_dict(), 'best_NCLA.pkl')\n",
    "    else:\n",
    "        counter += 1\n",
    "\n",
    "    if counter >= early_stop_counter:\n",
    "        print('early stop')\n",
    "        break\n",
    "\n",
    "    if epoch >= 0:\n",
    "        dur.append(time.time() - t0)\n",
    "\n",
    "    print(\"Epoch {:04d} | Time(s) {:.4f} | TrainLoss {:.4f} \".\n",
    "          format(epoch + 1, np.mean(dur), loss_train.item()))\n",
    "\n",
    "print('Loading {}th epoch'.format(best_t))\n",
    "model.load_state_dict(torch.load('best_NCLA.pkl'))#训练结束后，从最佳轮次加载模型参数 \n",
    "model.eval()#将模型设置为评估模式，并通过模型传递特征以获得嵌入\n",
    "with torch.no_grad():\n",
    "    heads = model(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('GATConv层输入：',in_dim, num_hidden, heads[0],\n",
    "            feat_drop, attn_drop, negative_slope, False, self.activation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [-0.4445, -0.0132,  0.2077,  ..., -0.2958, -0.2379, -0.0523],\n",
       "          [-0.1771, -0.5300,  0.2608,  ..., -0.3066, -0.5625,  0.2906],\n",
       "          [ 0.4795, -0.3555, -0.0991,  ..., -0.2951, -0.0933, -0.5426]],\n",
       " \n",
       "         [[ 0.3860, -0.1873,  0.2311,  ...,  0.3187, -0.2169, -0.3955],\n",
       "          [-0.0424, -0.0533, -0.1645,  ...,  0.0020, -0.1539, -0.1314],\n",
       "          [-0.1668, -0.0537, -0.0587,  ..., -0.0556, -0.1276,  0.0428],\n",
       "          [ 0.0868, -0.0298, -0.0330,  ...,  0.0730, -0.1953, -0.0856]],\n",
       " \n",
       "         [[-0.2173, -0.1561,  0.1978,  ...,  0.0582, -0.1489,  0.1396],\n",
       "          [ 0.1280, -0.3042, -0.5491,  ...,  0.2454, -0.0274, -0.1619],\n",
       "          [-0.2521,  0.0945, -0.1521,  ..., -0.0140, -0.0798,  0.0476],\n",
       "          [ 0.0059, -0.1211, -0.3256,  ..., -0.0262, -0.1508, -0.1379]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[-0.0097,  0.1236, -0.0399,  ..., -0.1214,  0.1626, -0.0693],\n",
       "          [ 0.0592,  0.0488, -0.2379,  ...,  0.0690,  0.0174, -0.2193],\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0207,  0.3500,  0.0438,  ...,  0.0776,  0.2349,  0.4670]],\n",
       " \n",
       "         [[-0.2618, -0.1843,  0.0120,  ...,  0.2215,  0.0985, -0.1779],\n",
       "          [-0.0333,  0.0214, -0.1201,  ..., -0.3588, -0.0245, -0.0013],\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [-0.2550,  0.0923,  0.1767,  ...,  0.3219,  0.2851, -0.0752]],\n",
       " \n",
       "         [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [-0.1521,  0.4014,  0.1154,  ..., -0.3352, -0.3019,  0.1271],\n",
       "          [ 0.0915,  0.1232,  0.1483,  ..., -0.2457, -0.2612, -0.2496],\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]],\n",
       "        grad_fn=<EluBackward0>)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "augmented_graph_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32md:\\Desktop\\毕业论文\\NCLA-main\\NCLA-main\\Untitled-1.ipynb 单元格 5\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Desktop/%E6%AF%95%E4%B8%9A%E8%AE%BA%E6%96%87/NCLA-main/NCLA-main/Untitled-1.ipynb#W4sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m augmented_graph_features\u001b[39m.\u001b[39;49mshape()\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "augmented_graph_features.shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(augmented_graph_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "only one element tensors can be converted to Python scalars",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32md:\\Desktop\\毕业论文\\NCLA-main\\NCLA-main\\Untitled-1.ipynb 单元格 7\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Desktop/%E6%AF%95%E4%B8%9A%E8%AE%BA%E6%96%87/NCLA-main/NCLA-main/Untitled-1.ipynb#W6sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m my_tensor \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mtensor(augmented_graph_features)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Desktop/%E6%AF%95%E4%B8%9A%E8%AE%BA%E6%96%87/NCLA-main/NCLA-main/Untitled-1.ipynb#W6sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m# 查看张量的长度\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: only one element tensors can be converted to Python scalars"
     ]
    }
   ],
   "source": [
    "my_tensor = torch.tensor(augmented_graph_features)\n",
    "\n",
    "# 查看张量的长度\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor Size: torch.Size([1, 3327, 4, 32])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 假设 augmented_graph_features 是一个列表，其中包含多个张量\n",
    "\n",
    "# 使用 torch.stack() 将列表中的张量堆叠成一个新的张量\n",
    "my_tensor = torch.stack(augmented_graph_features)\n",
    "\n",
    "# 查看张量的尺寸\n",
    "print(\"Tensor Size:\", my_tensor.size())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [-0.4445, -0.0132,  0.2077,  ..., -0.2958, -0.2379, -0.0523],\n",
       "          [-0.1771, -0.5300,  0.2608,  ..., -0.3066, -0.5625,  0.2906],\n",
       "          [ 0.4795, -0.3555, -0.0991,  ..., -0.2951, -0.0933, -0.5426]],\n",
       "\n",
       "         [[ 0.3860, -0.1873,  0.2311,  ...,  0.3187, -0.2169, -0.3955],\n",
       "          [-0.0424, -0.0533, -0.1645,  ...,  0.0020, -0.1539, -0.1314],\n",
       "          [-0.1668, -0.0537, -0.0587,  ..., -0.0556, -0.1276,  0.0428],\n",
       "          [ 0.0868, -0.0298, -0.0330,  ...,  0.0730, -0.1953, -0.0856]],\n",
       "\n",
       "         [[-0.2173, -0.1561,  0.1978,  ...,  0.0582, -0.1489,  0.1396],\n",
       "          [ 0.1280, -0.3042, -0.5491,  ...,  0.2454, -0.0274, -0.1619],\n",
       "          [-0.2521,  0.0945, -0.1521,  ..., -0.0140, -0.0798,  0.0476],\n",
       "          [ 0.0059, -0.1211, -0.3256,  ..., -0.0262, -0.1508, -0.1379]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-0.0097,  0.1236, -0.0399,  ..., -0.1214,  0.1626, -0.0693],\n",
       "          [ 0.0592,  0.0488, -0.2379,  ...,  0.0690,  0.0174, -0.2193],\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0207,  0.3500,  0.0438,  ...,  0.0776,  0.2349,  0.4670]],\n",
       "\n",
       "         [[-0.2618, -0.1843,  0.0120,  ...,  0.2215,  0.0985, -0.1779],\n",
       "          [-0.0333,  0.0214, -0.1201,  ..., -0.3588, -0.0245, -0.0013],\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [-0.2550,  0.0923,  0.1767,  ...,  0.3219,  0.2851, -0.0752]],\n",
       "\n",
       "         [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [-0.1521,  0.4014,  0.1154,  ..., -0.3352, -0.3019,  0.1271],\n",
       "          [ 0.0915,  0.1232,  0.1483,  ..., -0.2457, -0.2612, -0.2496],\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]]],\n",
       "       grad_fn=<StackBackward0>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torchmeta'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32md:\\Desktop\\毕业论文\\NCLA-main\\NCLA-main\\Untitled-1.ipynb 单元格 12\u001b[0m line \u001b[0;36m6\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Desktop/%E6%AF%95%E4%B8%9A%E8%AE%BA%E6%96%87/NCLA-main/NCLA-main/Untitled-1.ipynb#X14sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch_geometric\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mnn\u001b[39;00m \u001b[39mimport\u001b[39;00m GCNConv\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Desktop/%E6%AF%95%E4%B8%9A%E8%AE%BA%E6%96%87/NCLA-main/NCLA-main/Untitled-1.ipynb#X14sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch_geometric\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdatasets\u001b[39;00m \u001b[39mimport\u001b[39;00m Planetoid\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Desktop/%E6%AF%95%E4%B8%9A%E8%AE%BA%E6%96%87/NCLA-main/NCLA-main/Untitled-1.ipynb#X14sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorchmeta\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdatasets\u001b[39;00m \u001b[39mimport\u001b[39;00m Cora\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Desktop/%E6%AF%95%E4%B8%9A%E8%AE%BA%E6%96%87/NCLA-main/NCLA-main/Untitled-1.ipynb#X14sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorchmeta\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtransforms\u001b[39;00m \u001b[39mimport\u001b[39;00m ClassSplitter, Categorical\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Desktop/%E6%AF%95%E4%B8%9A%E8%AE%BA%E6%96%87/NCLA-main/NCLA-main/Untitled-1.ipynb#X14sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorchmeta\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m \u001b[39mimport\u001b[39;00m BatchMetaDataLoader\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torchmeta'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from torchmeta.datasets import Cora\n",
    "from torchmeta.transforms import ClassSplitter, Categorical\n",
    "from torchmeta.utils.data import BatchMetaDataLoader\n",
    "from torchmeta.modules import MetaModule\n",
    "from torchmeta.modules.utils import get_subdict\n",
    "\n",
    "# Define a simple graph neural network model\n",
    "class GraphNet(MetaModule):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(GraphNet, self).__init__()\n",
    "        self.conv1 = GCNConv(input_dim, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, hidden_dim)\n",
    "        self.conv3 = GCNConv(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = F.relu(self.conv2(x, edge_index))\n",
    "        x = self.conv3(x, edge_index)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "# Define a model that wraps the GraphNet for MAML\n",
    "class MetaGraphNet(MetaModule):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(MetaGraphNet, self).__init__()\n",
    "        self.graphnet = GraphNet(input_dim, hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        return self.graphnet(x, edge_index)\n",
    "\n",
    "# Define MAML algorithm\n",
    "class MAML:\n",
    "    def __init__(self, model, lr_inner=0.01, lr_outer=0.001):\n",
    "        self.model = model\n",
    "        self.lr_inner = lr_inner\n",
    "        self.lr_outer = lr_outer\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=lr_outer)\n",
    "\n",
    "    def inner_update(self, task):\n",
    "        support_x, support_y = task['train']\n",
    "        query_x, query_y = task['test']\n",
    "\n",
    "        # Clone the model to initialize inner loop parameters\n",
    "        inner_model = get_subdict(self.model, ['graphnet']).train()\n",
    "        \n",
    "        # Perform inner loop updates on support set\n",
    "        for _ in range(num_inner_updates):\n",
    "            logits = inner_model(support_x, task['edge_index'])\n",
    "            loss = F.nll_loss(logits, support_y)\n",
    "            self.model.zero_grad()\n",
    "            grads = torch.autograd.grad(loss, inner_model.parameters())\n",
    "            inner_model.update_params(lr=self.lr_inner, source_params=grads)\n",
    "\n",
    "        # Compute loss on query set\n",
    "        query_logits = inner_model(query_x, task['edge_index'])\n",
    "        loss = F.nll_loss(query_logits, query_y)\n",
    "        return loss\n",
    "\n",
    "    def outer_update(self, tasks):\n",
    "        meta_loss = torch.stack([self.inner_update(task) for task in tasks]).mean()\n",
    "        self.optimizer.zero_grad()\n",
    "        meta_loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "# Load Cora dataset\n",
    "dataset = Cora('data', num_classes_per_task=5, meta_train=True, meta_val=True, meta_test=False)\n",
    "dataset = ClassSplitter(shuffle=True, num_train_per_class=1, num_test_per_class=10)(dataset)\n",
    "meta_dataset = BatchMetaDataLoader(dataset, batch_size=16)\n",
    "\n",
    "# Initialize model for MAML\n",
    "model = MetaGraphNet(input_dim=dataset.num_features, hidden_dim=64, output_dim=dataset.num_classes)\n",
    "maml = MAML(model)\n",
    "\n",
    "# Train meta-learning model\n",
    "num_epochs = 100\n",
    "num_inner_updates = 5\n",
    "for epoch in range(num_epochs):\n",
    "    for tasks in meta_dataset:\n",
    "        maml.outer_update(tasks)\n",
    "\n",
    "# Get enhanced adjacency matrix and node features\n",
    "enhanced_adj = []\n",
    "enhanced_features = []\n",
    "for task in meta_dataset:\n",
    "    support_data, _ = task['train']\n",
    "    enhanced_adj.append(support_data.edge_index)\n",
    "    enhanced_features.append(support_data.x)\n",
    "enhanced_adj = torch.stack(enhanced_adj)\n",
    "enhanced_features = torch.stack(enhanced_features)\n",
    "\n",
    "print(\"Enhanced adjacency matrix shape:\", enhanced_adj.shape)\n",
    "print(\"Enhanced node features shape:\", enhanced_features.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchmetaNote: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Cannot install torchmeta==1.1.0, torchmeta==1.1.1, torchmeta==1.2.0, torchmeta==1.2.1, torchmeta==1.2.2, torchmeta==1.3.0, torchmeta==1.3.1, torchmeta==1.3.2, torchmeta==1.3.3, torchmeta==1.3.4, torchmeta==1.4.0, torchmeta==1.4.1, torchmeta==1.4.2, torchmeta==1.4.3, torchmeta==1.4.4, torchmeta==1.4.5, torchmeta==1.4.6, torchmeta==1.5.0, torchmeta==1.5.1, torchmeta==1.5.2, torchmeta==1.5.3, torchmeta==1.6.0, torchmeta==1.6.1, torchmeta==1.7.0 and torchmeta==1.8.0 because these package versions have conflicting dependencies.\n",
      "ERROR: ResolutionImpossible: for help visit https://pip.pypa.io/en/latest/topics/dependency-resolution/#dealing-with-dependency-conflicts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Downloading torchmeta-1.8.0-py3-none-any.whl (210 kB)\n",
      "     ---------------------------------------- 0.0/210.4 kB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/210.4 kB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/210.4 kB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/210.4 kB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/210.4 kB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/210.4 kB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/210.4 kB ? eta -:--:--\n",
      "     - -------------------------------------- 10.2/210.4 kB ? eta -:--:--\n",
      "     - -------------------------------------- 10.2/210.4 kB ? eta -:--:--\n",
      "     ----- ------------------------------- 30.7/210.4 kB 262.6 kB/s eta 0:00:01\n",
      "     ------- ----------------------------- 41.0/210.4 kB 281.8 kB/s eta 0:00:01\n",
      "     ------- ----------------------------- 41.0/210.4 kB 281.8 kB/s eta 0:00:01\n",
      "     ------- ----------------------------- 41.0/210.4 kB 281.8 kB/s eta 0:00:01\n",
      "     ------- ----------------------------- 41.0/210.4 kB 281.8 kB/s eta 0:00:01\n",
      "     ------- ----------------------------- 41.0/210.4 kB 281.8 kB/s eta 0:00:01\n",
      "     ------- ----------------------------- 41.0/210.4 kB 281.8 kB/s eta 0:00:01\n",
      "     ------- ----------------------------- 41.0/210.4 kB 281.8 kB/s eta 0:00:01\n",
      "     ------- ----------------------------- 41.0/210.4 kB 281.8 kB/s eta 0:00:01\n",
      "     --------------------- -------------- 122.9/210.4 kB 212.1 kB/s eta 0:00:01\n",
      "     --------------------- -------------- 122.9/210.4 kB 212.1 kB/s eta 0:00:01\n",
      "     --------------------- -------------- 122.9/210.4 kB 212.1 kB/s eta 0:00:01\n",
      "     ------------------------------------ 210.4/210.4 kB 312.7 kB/s eta 0:00:00\n",
      "INFO: pip is looking at multiple versions of torchmeta to determine which version is compatible with other requirements. This could take a while.\n",
      "  Downloading torchmeta-1.7.0-py3-none-any.whl (181 kB)\n",
      "     ---------------------------------------- 0.0/181.4 kB ? eta -:--:--\n",
      "     -------- ------------------------------ 41.0/181.4 kB 1.9 MB/s eta 0:00:01\n",
      "     -------------- ---------------------- 71.7/181.4 kB 991.0 kB/s eta 0:00:01\n",
      "     ------------------------------ ------- 143.4/181.4 kB 1.1 MB/s eta 0:00:01\n",
      "     -------------------------------------- 181.4/181.4 kB 1.2 MB/s eta 0:00:00\n",
      "  Downloading torchmeta-1.6.1-py3-none-any.whl (181 kB)\n",
      "     ---------------------------------------- 0.0/181.4 kB ? eta -:--:--\n",
      "     ------------------------------------- 181.4/181.4 kB 10.7 MB/s eta 0:00:00\n",
      "  Downloading torchmeta-1.6.0-py3-none-any.whl (179 kB)\n",
      "     ---------------------------------------- 0.0/180.0 kB ? eta -:--:--\n",
      "     ------------- ------------------------- 61.4/180.0 kB 3.2 MB/s eta 0:00:01\n",
      "     ------------------- ------------------- 92.2/180.0 kB 5.1 MB/s eta 0:00:01\n",
      "     ------------------- ------------------- 92.2/180.0 kB 5.1 MB/s eta 0:00:01\n",
      "     ------------------- ------------------- 92.2/180.0 kB 5.1 MB/s eta 0:00:01\n",
      "     ------------------- ------------------- 92.2/180.0 kB 5.1 MB/s eta 0:00:01\n",
      "     ------------------- ------------------- 92.2/180.0 kB 5.1 MB/s eta 0:00:01\n",
      "     ------------------- ------------------- 92.2/180.0 kB 5.1 MB/s eta 0:00:01\n",
      "     ------------------- ------------------- 92.2/180.0 kB 5.1 MB/s eta 0:00:01\n",
      "     ------------------- ------------------- 92.2/180.0 kB 5.1 MB/s eta 0:00:01\n",
      "     ------------------- ------------------- 92.2/180.0 kB 5.1 MB/s eta 0:00:01\n",
      "     ---------------------------------- - 174.1/180.0 kB 361.7 kB/s eta 0:00:01\n",
      "     ------------------------------------ 180.0/180.0 kB 361.9 kB/s eta 0:00:00\n",
      "  Downloading torchmeta-1.5.3-py3-none-any.whl (179 kB)\n",
      "     ---------------------------------------- 0.0/179.7 kB ? eta -:--:--\n",
      "     ------ --------------------------------- 30.7/179.7 kB ? eta -:--:--\n",
      "     -------------- ---------------------- 71.7/179.7 kB 975.2 kB/s eta 0:00:01\n",
      "     ------------------------- ------------ 122.9/179.7 kB 1.0 MB/s eta 0:00:01\n",
      "     -------------------------------- ----- 153.6/179.7 kB 1.0 MB/s eta 0:00:01\n",
      "     -------------------------------------- 179.7/179.7 kB 1.1 MB/s eta 0:00:00\n",
      "  Downloading torchmeta-1.5.2-py3-none-any.whl (179 kB)\n",
      "     ---------------------------------------- 0.0/179.5 kB ? eta -:--:--\n",
      "     ------------- ------------------------- 61.4/179.5 kB 1.6 MB/s eta 0:00:01\n",
      "     -------------------------- ----------- 122.9/179.5 kB 1.8 MB/s eta 0:00:01\n",
      "     -------------------------------------- 179.5/179.5 kB 1.5 MB/s eta 0:00:00\n",
      "  Downloading torchmeta-1.5.1-py3-none-any.whl (175 kB)\n",
      "     ---------------------------------------- 0.0/175.7 kB ? eta -:--:--\n",
      "     --------------- ----------------------- 71.7/175.7 kB 3.8 MB/s eta 0:00:01\n",
      "     ------------------------------- ------ 143.4/175.7 kB 2.1 MB/s eta 0:00:01\n",
      "     -------------------------------------- 175.7/175.7 kB 1.8 MB/s eta 0:00:00\n",
      "  Downloading torchmeta-1.5.0-py3-none-any.whl (172 kB)\n",
      "     ---------------------------------------- 0.0/172.6 kB ? eta -:--:--\n",
      "     -------------------- ------------------ 92.2/172.6 kB 5.5 MB/s eta 0:00:01\n",
      "     ------------------------------- ------ 143.4/172.6 kB 2.1 MB/s eta 0:00:01\n",
      "     -------------------------------------- 172.6/172.6 kB 1.7 MB/s eta 0:00:00\n",
      "INFO: pip is still looking at multiple versions of torchmeta to determine which version is compatible with other requirements. This could take a while.\n",
      "  Downloading torchmeta-1.4.6-py3-none-any.whl (171 kB)\n",
      "     ---------------------------------------- 0.0/171.3 kB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/171.3 kB ? eta -:--:--\n",
      "     -------------------------------------- 171.3/171.3 kB 5.2 MB/s eta 0:00:00\n",
      "  Downloading torchmeta-1.4.5-py3-none-any.whl (168 kB)\n",
      "     ---------------------------------------- 0.0/168.2 kB ? eta -:--:--\n",
      "     -------------- ------------------------ 61.4/168.2 kB 3.2 MB/s eta 0:00:01\n",
      "     -------------------------------------  163.8/168.2 kB 2.4 MB/s eta 0:00:01\n",
      "     -------------------------------------- 168.2/168.2 kB 2.0 MB/s eta 0:00:00\n",
      "  Downloading torchmeta-1.4.4-py3-none-any.whl (167 kB)\n",
      "     ---------------------------------------- 0.0/167.6 kB ? eta -:--:--\n",
      "     --------- ----------------------------- 41.0/167.6 kB 1.9 MB/s eta 0:00:01\n",
      "     -------------------------------------- 167.6/167.6 kB 2.5 MB/s eta 0:00:00\n",
      "  Downloading torchmeta-1.4.3-py3-none-any.whl (165 kB)\n",
      "     ---------------------------------------- 0.0/165.3 kB ? eta -:--:--\n",
      "     --------- ----------------------------- 41.0/165.3 kB 1.9 MB/s eta 0:00:01\n",
      "     ------------------- ------------------- 81.9/165.3 kB 1.2 MB/s eta 0:00:01\n",
      "     -------------------------------------- 165.3/165.3 kB 1.2 MB/s eta 0:00:00\n",
      "  Downloading torchmeta-1.4.2-py3-none-any.whl (161 kB)\n",
      "     ---------------------------------------- 0.0/161.4 kB ? eta -:--:--\n",
      "     --------- ----------------------------- 41.0/161.4 kB 2.0 MB/s eta 0:00:01\n",
      "     ---------------------- ---------------- 92.2/161.4 kB 1.1 MB/s eta 0:00:01\n",
      "     -------------------------------------- 161.4/161.4 kB 1.4 MB/s eta 0:00:00\n",
      "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "  Downloading torchmeta-1.4.1-py3-none-any.whl (160 kB)\n",
      "     ---------------------------------------- 0.0/160.4 kB ? eta -:--:--\n",
      "     ---------------------- ---------------- 92.2/160.4 kB 2.6 MB/s eta 0:00:01\n",
      "     -------------------------------------- 160.4/160.4 kB 1.9 MB/s eta 0:00:00\n",
      "  Downloading torchmeta-1.4.0-py3-none-any.whl (155 kB)\n",
      "     ---------------------------------------- 0.0/155.5 kB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/155.5 kB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/155.5 kB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/155.5 kB ? eta -:--:--\n",
      "     -------------------------------------- 155.5/155.5 kB 9.1 MB/s eta 0:00:00\n",
      "  Downloading torchmeta-1.3.4-py3-none-any.whl (154 kB)\n",
      "     ---------------------------------------- 0.0/154.8 kB ? eta -:--:--\n",
      "     --------------- ----------------------- 61.4/154.8 kB 3.2 MB/s eta 0:00:01\n",
      "     -------------------------------------- 154.8/154.8 kB 2.3 MB/s eta 0:00:00\n",
      "  Downloading torchmeta-1.3.3-py3-none-any.whl (145 kB)\n",
      "     ---------------------------------------- 0.0/145.7 kB ? eta -:--:--\n",
      "     ---------------- ---------------------- 61.4/145.7 kB 3.2 MB/s eta 0:00:01\n",
      "     -------------------------------------  143.4/145.7 kB 1.7 MB/s eta 0:00:01\n",
      "     -------------------------------------- 145.7/145.7 kB 1.4 MB/s eta 0:00:00\n",
      "  Downloading torchmeta-1.3.2-py3-none-any.whl (145 kB)\n",
      "     ---------------------------------------- 0.0/145.0 kB ? eta -:--:--\n",
      "     ----------------------------- -------- 112.6/145.0 kB 3.3 MB/s eta 0:00:01\n",
      "     -------------------------------------- 145.0/145.0 kB 2.9 MB/s eta 0:00:00\n",
      "  Downloading torchmeta-1.3.1-py3-none-any.whl (144 kB)\n",
      "     ---------------------------------------- 0.0/144.8 kB ? eta -:--:--\n",
      "     ---------------- ---------------------- 61.4/144.8 kB 1.7 MB/s eta 0:00:01\n",
      "     -------------------------------------- 144.8/144.8 kB 1.7 MB/s eta 0:00:00\n",
      "  Downloading torchmeta-1.3.0-py3-none-any.whl (144 kB)\n",
      "     ---------------------------------------- 0.0/144.7 kB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/144.7 kB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/144.7 kB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/144.7 kB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/144.7 kB ? eta -:--:--\n",
      "     -- ------------------------------------- 10.2/144.7 kB ? eta -:--:--\n",
      "     -- ------------------------------------- 10.2/144.7 kB ? eta -:--:--\n",
      "     -- ------------------------------------- 10.2/144.7 kB ? eta -:--:--\n",
      "     ------- ----------------------------- 30.7/144.7 kB 130.4 kB/s eta 0:00:01\n",
      "     ------- ----------------------------- 30.7/144.7 kB 130.4 kB/s eta 0:00:01\n",
      "     ------- ----------------------------- 30.7/144.7 kB 130.4 kB/s eta 0:00:01\n",
      "     ------- ----------------------------- 30.7/144.7 kB 130.4 kB/s eta 0:00:01\n",
      "     ------- ----------------------------- 30.7/144.7 kB 130.4 kB/s eta 0:00:01\n",
      "     ------- ----------------------------- 30.7/144.7 kB 130.4 kB/s eta 0:00:01\n",
      "     ------- ----------------------------- 30.7/144.7 kB 130.4 kB/s eta 0:00:01\n",
      "     ---------- --------------------------- 41.0/144.7 kB 61.4 kB/s eta 0:00:02\n",
      "     ---------- --------------------------- 41.0/144.7 kB 61.4 kB/s eta 0:00:02\n",
      "     ---------- --------------------------- 41.0/144.7 kB 61.4 kB/s eta 0:00:02\n",
      "     ---------- --------------------------- 41.0/144.7 kB 61.4 kB/s eta 0:00:02\n",
      "     ------------------ ------------------- 71.7/144.7 kB 91.4 kB/s eta 0:00:01\n",
      "     ------------------ ------------------- 71.7/144.7 kB 91.4 kB/s eta 0:00:01\n",
      "     ----------------------- ------------- 92.2/144.7 kB 102.8 kB/s eta 0:00:01\n",
      "     ---------------------------- ------- 112.6/144.7 kB 121.3 kB/s eta 0:00:01\n",
      "     ------------------------------------ 144.7/144.7 kB 153.7 kB/s eta 0:00:00\n",
      "  Downloading torchmeta-1.2.2-py3-none-any.whl (139 kB)\n",
      "     ---------------------------------------- 0.0/139.6 kB ? eta -:--:--\n",
      "     ----------------- --------------------- 61.4/139.6 kB 1.7 MB/s eta 0:00:01\n",
      "     -------------------------------------- 139.6/139.6 kB 1.4 MB/s eta 0:00:00\n",
      "  Downloading torchmeta-1.2.1-py3-none-any.whl (139 kB)\n",
      "     ---------------------------------------- 0.0/139.6 kB ? eta -:--:--\n",
      "     ------------------------------------ - 133.1/139.6 kB 2.6 MB/s eta 0:00:01\n",
      "     -------------------------------------- 139.6/139.6 kB 2.1 MB/s eta 0:00:00\n",
      "  Downloading torchmeta-1.2.0-py3-none-any.whl (139 kB)\n",
      "     ---------------------------------------- 0.0/139.6 kB ? eta -:--:--\n",
      "     ---------------------- ---------------- 81.9/139.6 kB 4.5 MB/s eta 0:00:01\n",
      "     -------------------------------------- 139.6/139.6 kB 2.1 MB/s eta 0:00:00\n",
      "  Downloading torchmeta-1.1.1-py3-none-any.whl (133 kB)\n",
      "     ---------------------------------------- 0.0/133.1 kB ? eta -:--:--\n",
      "     -------------------------------------- 133.1/133.1 kB 7.7 MB/s eta 0:00:00\n",
      "  Downloading torchmeta-1.1.0-py3-none-any.whl (133 kB)\n",
      "     ---------------------------------------- 0.0/133.0 kB ? eta -:--:--\n",
      "     ------------------ -------------------- 61.4/133.0 kB 3.2 MB/s eta 0:00:01\n",
      "     --------------------------- ----------- 92.2/133.0 kB 1.7 MB/s eta 0:00:01\n",
      "     -------------------------------------- 133.0/133.0 kB 1.3 MB/s eta 0:00:00\n",
      "\n",
      "The conflict is caused by:\n",
      "    torchmeta 1.8.0 depends on torch<1.10.0 and >=1.4.0\n",
      "    torchmeta 1.7.0 depends on torch<1.9.0 and >=1.4.0\n",
      "    torchmeta 1.6.1 depends on torch<1.8.0 and >=1.4.0\n",
      "    torchmeta 1.6.0 depends on torch<1.8.0 and >=1.4.0\n",
      "    torchmeta 1.5.3 depends on torch<1.7.0 and >=1.4.0\n",
      "    torchmeta 1.5.2 depends on torch<1.7.0 and >=1.4.0\n",
      "    torchmeta 1.5.1 depends on torch<1.7.0 and >=1.4.0\n",
      "    torchmeta 1.5.0 depends on torch<1.7.0 and >=1.4.0\n",
      "    torchmeta 1.4.6 depends on torch<1.6.0 and >=1.4.0\n",
      "    torchmeta 1.4.5 depends on torch<1.6.0 and >=1.4.0\n",
      "    torchmeta 1.4.4 depends on torch<1.6.0 and >=1.4.0\n",
      "    torchmeta 1.4.3 depends on torch<1.6.0 and >=1.4.0\n",
      "    torchmeta 1.4.2 depends on torch<1.6.0 and >=1.4.0\n",
      "    torchmeta 1.4.1 depends on torch<1.5.0 and >=1.4.0\n",
      "    torchmeta 1.4.0 depends on torch<1.5.0 and >=1.4.0\n",
      "    torchmeta 1.3.4 depends on torch<1.5.0 and >=1.4.0\n",
      "    torchmeta 1.3.3 depends on torch<1.5.0 and >=1.4.0\n",
      "    torchmeta 1.3.2 depends on torch<1.5.0 and >=1.4.0\n",
      "    torchmeta 1.3.1 depends on torch<1.5.0 and >=1.3.0\n",
      "    torchmeta 1.3.0 depends on torch<1.5.0 and >=1.3.0\n",
      "    torchmeta 1.2.2 depends on torch<1.4.0 and >=1.3.0\n",
      "    torchmeta 1.2.1 depends on torch<1.4.0 and >=1.3.0\n",
      "    torchmeta 1.2.0 depends on torch<1.3.0 and >=1.2.0\n",
      "    torchmeta 1.1.1 depends on torch<1.3.0 and >=1.2.0\n",
      "    torchmeta 1.1.0 depends on torch<1.3.0 and >=1.2.0\n",
      "\n",
      "To fix this you could try to:\n",
      "1. loosen the range of package versions you've specified\n",
      "2. remove package versions to allow pip attempt to solve the dependency conflict\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# pip install torchmeta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Package(s) not found: torchmeta\n"
     ]
    }
   ],
   "source": [
    "pip show torchmeta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
